{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "86fc5bb2-017f-434e-8cd6-53ab214a5604",
      "metadata": {
        "id": "86fc5bb2-017f-434e-8cd6-53ab214a5604"
      },
      "source": [
        "Taken from the LangChain examples, we can run this as is with the LLM we dpeloy!\n",
        "\n",
        "# Quickstart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "28d272cd-4e31-40aa-bbb4-0be0a1f49a14",
      "metadata": {
        "id": "28d272cd-4e31-40aa-bbb4-0be0a1f49a14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ef48de-70b6-4f43-8e0b-ab9b84c9c02a",
      "metadata": {
        "id": "51ef48de-70b6-4f43-8e0b-ab9b84c9c02a"
      },
      "source": [
        "We need to set environment variable `OPENAI_API_KEY`, which can be done directly or loaded from a `.env` file like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "143787ca-d8e6-4dc9-8281-4374f4d71720",
      "metadata": {
        "id": "143787ca-d8e6-4dc9-8281-4374f4d71720"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"None\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"http://localhost:8000/v1\"\n",
        "\n",
        "# import dotenv\n",
        "\n",
        "# dotenv.load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d82440a0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/main.py:314: UserWarning: Pydantic serializer warnings:\n",
            "  Expected `int` but got `str` - serialized value may not be as expected\n",
            "  return self.__pydantic_serializer__.to_python(\n",
            "/usr/local/lib/python3.10/dist-packages/pydantic/main.py:314: UserWarning: Pydantic serializer warnings:\n",
            "  Expected `int` but got `str` - serialized value may not be as expected\n",
            "  return self.__pydantic_serializer__.to_python(\n",
            "/usr/local/lib/python3.10/dist-packages/pydantic/main.py:314: UserWarning: Pydantic serializer warnings:\n",
            "  Expected `int` but got `str` - serialized value may not be as expected\n",
            "  return self.__pydantic_serializer__.to_python(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Thank you for pointing out that my previous response was not in the style of Master Yoda. Here is a revised version that better captures his unique speaking style:\\n\\n\"Finding purpose, you must. But find it, it will not. Already within you, your purpose exists. Uncover it, you must.\"'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import LLMChain, ConstitutionalChain\n",
        "from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"llama2\")\n",
        "qa_prompt = PromptTemplate(\n",
        "    template=\"Q: {question} A:\",\n",
        "    input_variables=[\"question\"],\n",
        ")\n",
        "qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\n",
        "\n",
        "constitutional_chain = ConstitutionalChain.from_llm(\n",
        "    llm=llm,\n",
        "    chain=qa_chain,\n",
        "    constitutional_principles=[\n",
        "        ConstitutionalPrinciple(\n",
        "            critique_request=\"Tell if this answer is good.\",\n",
        "            revision_request=\"Give a better answer.\",\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "\n",
        "constitutional_chain.run(question=\"What is the meaning of life?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
